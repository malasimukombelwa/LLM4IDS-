# -*- coding: utf-8 -*-
"""llm4ids-multiclasses.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N2o4VXby70qoHyBIpinkiqgDhpZZg7n7
"""

from google.colab import drive
drive.mount('/content/drive')

#!pip install -q sentence-transformers

"""**LLM4IDS Multiclasse** :

Input : NSL-KDD (données tabulaires + texte)

→ Prétraitement :
    - Texte généré à partir de chaque ligne (row_to_text)
    
    - Standardisation des features numériques (StandardScaler)
    
    - Encodage du label avec LabelEncoder

→ Deux modalités :

    [1] Embeddings textuels via SentenceTransformer ("all-MiniLM-L6-v2")
    
    [2] Caractéristiques numériques (39 colonnes NSL-KDD normalisées)

→ Fusion :

    - Concatenation de X_text_emb (384 dim) + X_num (39 dim) → X_comb (423 dim)

→ Modèle :
    - Transfomer avec 200 arbre
    profondeur 6, objectif softmax
    - Apprentissage sur X_comb
    - Classification en 5 classes : ['DoS', 'Probe', 'R2L', 'U2R', 'normal']

→ Sortie :
    - Prédictions discrètes (0 à 4) converties en noms de classes
    - Évaluation : classification_report, confusion_matrix, ROC possible

"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.preprocessing import StandardScaler
from sentence_transformers import SentenceTransformer
import joblib
import matplotlib.pyplot as plt

# 1. Colonnes NSL-KDD
columns = [
    "duration", "protocol_type", "service", "flag", "src_bytes", "dst_bytes",
    "land", "wrong_fragment", "urgent", "hot", "num_failed_logins", "logged_in",
    "num_compromised", "root_shell", "su_attempted", "num_root", "num_file_creations",
    "num_shells", "num_access_files", "num_outbound_cmds", "is_host_login", "is_guest_login",
    "count", "srv_count", "serror_rate", "srv_serror_rate", "rerror_rate", "srv_rerror_rate",
    "same_srv_rate", "diff_srv_rate", "srv_diff_host_rate", "dst_host_count",
    "dst_host_srv_count", "dst_host_same_srv_rate", "dst_host_diff_srv_rate",
    "dst_host_same_src_port_rate", "dst_host_srv_diff_host_rate", "dst_host_serror_rate",
    "dst_host_srv_serror_rate", "dst_host_rerror_rate", "dst_host_srv_rerror_rate",
    "label", "difficulty"
]

# 2. Regroupement des étiquettes
attack_mapping = {
    'normal': 'normal',
    'neptune': 'DoS', 'smurf': 'DoS', 'teardrop': 'DoS', 'pod': 'DoS', 'back': 'DoS',
    'apache2': 'DoS', 'udpstorm': 'DoS', 'processtable': 'DoS', 'mailbomb': 'DoS',
    'ipsweep': 'Probe', 'portsweep': 'Probe', 'nmap': 'Probe', 'satan': 'Probe',
    'mscan': 'Probe', 'saint': 'Probe',
    'guess_passwd': 'R2L', 'ftp_write': 'R2L', 'imap': 'R2L', 'phf': 'R2L',
    'multihop': 'R2L', 'warezmaster': 'R2L', 'warezclient': 'R2L', 'spy': 'R2L',
    'sendmail': 'R2L', 'named': 'R2L', 'snmpgetattack': 'R2L', 'snmpguess': 'R2L',
    'httptunnel': 'R2L', 'xlock': 'R2L', 'xsnoop': 'R2L',
    'buffer_overflow': 'U2R', 'loadmodule': 'U2R', 'rootkit': 'U2R',
    'perl': 'U2R', 'sqlattack': 'U2R', 'xterm': 'U2R', 'ps': 'U2R', 'worm': 'U2R',
    'land': 'DoS'
}

# 3. Fonction de transformation en texte
def row_to_text(row):
    return (
        f"{row['protocol_type']} connection using {row['service']} service with {row['flag']} flag. "
        f"Sent {row['src_bytes']} bytes and received {row['dst_bytes']} bytes. "
        f"Login status: {'successful' if row['logged_in'] == 1 else 'failed'}. "
        f"{'Root access attempted.' if row['root_shell'] == 1 else ''} "
        f"Connection classified as {row['label']}."
    )

# 4. Chargement des données
df_train = pd.read_csv("https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTrain%2B.txt", header=None, names=columns)
df_test = pd.read_csv("https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTest%2B.txt", header=None, names=columns)

# 5. Prétraitement
for df in [df_train, df_test]:
    df['label'] = df['label'].map(attack_mapping)
    df['text'] = df.apply(row_to_text, axis=1)

# 6. Séparation des variables
X_full = df_train['text']
y_full = df_train['label']
X_test_text = df_test['text']
y_test = df_test['label'].astype(str)

X_train_text, X_val_text, y_train, y_val = train_test_split(X_full, y_full, test_size=0.2, random_state=42)

print("Dimensions des ensembles :")
print("X_train_text.shape:", X_train_text.shape)
print("X_val_text.shape  :", X_val_text.shape)
print("X_test_text.shape :", X_test_text.shape)
print("y_train.shape     :", y_train.shape)
print("y_val.shape       :", y_val.shape)
print("y_test.shape      :", y_test.shape)

print("X_val_text[:5] →", X_val_text[:5])
print("y_val[:5]     →", y_val[:5])  # ← vérifie si les textes et labels correspondent

row_to_text(df_train.iloc[0])

df_train['label'].value_counts()

import matplotlib.pyplot as plt

# Function to plot class distribution pie chart
def plot_attack_distribution(df, title):
    expected_labels = ['normal', 'DoS', 'Probe', 'R2L', 'U2R']

    # Count occurrences, reindex to ensure all labels appear
    counts = df['label'].value_counts().reindex(expected_labels, fill_value=0)

    # Custom color palette
    colors = ['#00b894', '#d63031', '#0984e3', '#fdcb6e', '#6c5ce7']

    # Plot pie chart
    counts.plot.pie(
        autopct='%1.1f%%',
        startangle=90,
        colors=colors,
        labels=counts.index,
        wedgeprops={'edgecolor': 'white'}
    )

    plt.title(title, fontweight='bold')
    plt.ylabel("")  # Hide the y-axis label
    plt.tight_layout()
    plt.show()

# Plotting for training and testing datasets
plot_attack_distribution(df_train, "Distribution of Attack Categories (Train Set)")
plot_attack_distribution(df_test, "Distribution of Attack Categories (Test Set)")

# 7. Features numériques (standardisées)
num_cols = df_train.drop(columns=['protocol_type', 'service', 'flag', 'label', 'difficulty', 'text']).columns

X_train_num = df_train.loc[X_train_text.index][num_cols].values
X_val_num = df_train.loc[X_val_text.index][num_cols].values
X_test_num = df_test[num_cols].values

scaler = StandardScaler()
X_train_num = scaler.fit_transform(X_train_num)
X_val_num = scaler.transform(X_val_num)
X_test_num = scaler.transform(X_test_num)

print("Dimensions des ensembles :")
print("X_train_num.shape:", X_train_num.shape)
print("X_val_num.shape  :", X_val_num.shape)
print("X_test_num.shape :", X_test_num.shape)

"""## Embeddings textuels  SentenceTransformer all-MiniLM-L6-v2"""

# 8. Embeddings textuels  SentenceTransformer all-MiniLM-L6-v2
encoder = SentenceTransformer("all-MiniLM-L6-v2")
X_train_emb = encoder.encode(X_train_text.tolist(), show_progress_bar=True)
X_val_emb = encoder.encode(X_val_text.tolist(), show_progress_bar=True)
X_test_emb = encoder.encode(X_test_text.tolist(), show_progress_bar=True)

print("Dimensions des ensembles :")
print("X_train_emb.shape:", X_train_emb.shape)
print("X_val_emb.shape  :", X_val_emb.shape)
print("X_test_emb.shape :", X_test_emb.shape)

#Save data
import joblib
joblib.dump(X_train_emb, "/content/drive/MyDrive/SAINT-IDS/NSL-KDD/Data/X_train_emb1.pkl")
joblib.dump(X_val_emb, "/content/drive/MyDrive/SAINT-IDS/NSL-KDD/Data/X_val_emb1.pkl")
joblib.dump(X_test_emb, "/content/drive/MyDrive/SAINT-IDS/NSL-KDD/Data/X_test_emb1.pkl")

"""## Fusion de données"""

#Loard data
import joblib
X_train_emb = joblib.load("/content/drive/MyDrive/SAINT-IDS/NSL-KDD/Data/X_train_emb.pkl")
X_val_emb = joblib.load("/content/drive/MyDrive/SAINT-IDS/NSL-KDD/Data/X_val_emb.pkl")
X_test_emb = joblib.load("/content/drive/MyDrive/SAINT-IDS/NSL-KDD/Data/X_test_emb.pkl")

import joblib
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
import seaborn as sns

# Load embeddings and labels
X_train_emb = joblib.load("/content/drive/MyDrive/SAINT-IDS/NSL-KDD/Data/X_train_emb.pkl")
y_train = y_train  # Must match embeddings

# Apply t-SNE to reduce to 2D
tsne = TSNE(n_components=2, random_state=42, perplexity=30, learning_rate=200, n_iter=1000)
X_train_2D = tsne.fit_transform(X_train_emb)

# Convert to DataFrame for plotting
import pandas as pd
df_vis = pd.DataFrame(X_train_2D, columns=["TSNE-1", "TSNE-2"])
df_vis["Label"] = y_train.values if hasattr(y_train, 'values') else y_train

# Plot with Seaborn
plt.figure(figsize=(10, 7))
sns.scatterplot(data=df_vis, x="TSNE-1", y="TSNE-2", hue="Label", palette="Set2", alpha=0.8, s=60)
plt.title("t-SNE Visualization of LLM Embeddings (Train Set)", fontsize=14)
plt.legend(title="Class", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

import joblib
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
import seaborn as sns

# Load embeddings and labels
X_test_emb = joblib.load("/content/drive/MyDrive/SAINT-IDS/NSL-KDD/Data/X_test_emb.pkl")
y_test = y_test # Must match embeddings

# Apply t-SNE to reduce to 2D
tsne = TSNE(n_components=2, random_state=42, perplexity=30, learning_rate=200, n_iter=1000)
X_test_2D = tsne.fit_transform(X_test_emb)

# Convert to DataFrame for plotting
import pandas as pd
df_vis = pd.DataFrame(X_test_2D, columns=["TSNE-1", "TSNE-2"])
df_vis["Label"] = y_test.values if hasattr(y_test, 'values') else y_test

# Plot with Seaborn
plt.figure(figsize=(10, 7))
sns.scatterplot(data=df_vis, x="TSNE-1", y="TSNE-2", hue="Label", palette="Set2", alpha=0.8, s=60)
plt.title("t-SNE Visualization of LLM Embeddings (Test Set)", fontsize=14)
plt.legend(title="Class", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

# 9. Fusion des deux modalités
X_train_comb = np.concatenate([X_train_emb, X_train_num], axis=1)
X_val_comb = np.concatenate([X_val_emb, X_val_num], axis=1)
X_test_comb = np.concatenate([X_test_emb, X_test_num], axis=1)

print("Dimensions des ensembles :")
print("X_train_comb.shape:", X_train_comb.shape)
print("X_val_comb.shape  :", X_val_comb.shape)
print("X_test_comb.shape :", X_test_comb.shape)

from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical

# Encodage des étiquettes (str -> int -> one-hot)
le = LabelEncoder()
y_train_enc = to_categorical(le.fit_transform(y_train))
y_val_enc = to_categorical(le.transform(y_val))
y_test_enc = to_categorical(le.transform(y_test))

"""## Visualisation"""

# Affichage du shape (utile pour valider one-hot)
print("Dimensions :")
print("y_train_enc.shape:", y_train_enc.shape)
print("y_val_enc.shape  :", y_val_enc.shape)
print("y_test_enc.shape :", y_test_enc.shape)

# Affichage d'un extrait du contenu (premières lignes)
print("\n Extrait de y_train_enc :")
print(y_train_enc[:5])

print("\nExtrait de y_val_enc :")
print(y_val_enc[:5])

print("\n Extrait de y_test_enc :")
print(y_test_enc[:5])

# Dimensions
print("Dimensions des ensembles combinés :")
print("X_train_comb.shape :", X_train_comb.shape)
print("X_val_comb.shape   :", X_val_comb.shape)
print("X_test_comb.shape  :", X_test_comb.shape)

# Extraits de contenu (5 premières lignes)
print("\n Aperçu de X_train_comb :")
print(X_train_comb[:5])

print("\n Aperçu de X_val_comb :")
print(X_val_comb[:5])

print("\n Aperçu de X_test_comb :")
print(X_test_comb[:5])

"""## **Fonctiob E valuation**"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    classification_report, confusion_matrix,
    roc_curve, roc_auc_score
)
from sklearn.preprocessing import label_binarize

def eval_model_unv(model, X_test, y_test, le=None, model_name="Model"):
    """
    Évaluation universelle pour modèles Scikit-learn et Keras (multiclasse).
    Gère : y_test encodé (int), one-hot ou texte ; modèles avec predict ou predict_proba.
    """

    # 1. Gestion de y_test one-hot → int
    if len(y_test.shape) > 1 and y_test.shape[1] > 1:
        y_test = np.argmax(y_test, axis=1)

    # 2. Prédiction des scores
    try:
        y_score = model.predict_proba(X_test)
    except AttributeError:
        y_score = model.predict(X_test)

    # 3. Si y_score est 1D ou shape ≠ nb_classes → classif direct
    if len(y_score.shape) == 1 or y_score.shape[1] == 1:
        y_pred = (y_score > 0.5).astype(int)
    else:
        y_pred = np.argmax(y_score, axis=1)

    # 4. Gestion du LabelEncoder (optionnel)
    if le is not None:
        if isinstance(y_test[0], str):
            y_test_labels = y_test
            y_test_int = le.transform(y_test)
        else:
            y_test_int = y_test
            y_test_labels = le.inverse_transform(y_test)

        if isinstance(y_pred[0], str):
            y_pred_labels = y_pred
            y_pred_int = le.transform(y_pred)
        else:
            y_pred_int = y_pred
            y_pred_labels = le.inverse_transform(y_pred)

        class_names = le.classes_
    else:
        y_test_int = y_test
        y_pred_int = y_pred
        y_test_labels = y_test
        y_pred_labels = y_pred
        class_names = np.unique(y_test)

    # 5. Rapport de classification
    print(f"\n Rapport de classification - {model_name}")
    print(classification_report(y_test_labels, y_pred_labels, target_names=class_names))

    # 6. Matrice de confusion
    cm = confusion_matrix(y_test_labels, y_pred_labels, labels=class_names)
    plt.figure(figsize=(7, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.title(f"Matrice de confusion - {model_name}")
    plt.xlabel("Prédit")
    plt.ylabel("Réel")
    plt.tight_layout()
    plt.show()

    # 7. TPR / FPR
    print("\n TPR (Recall) et FPR par classe :")
    for i, name in enumerate(class_names):
        TP = cm[i, i]
        FN = np.sum(cm[i, :]) - TP
        FP = np.sum(cm[:, i]) - TP
        TN = np.sum(cm) - (TP + FN + FP)
        TPR = TP / (TP + FN) if (TP + FN) > 0 else 0.0
        FPR = FP / (FP + TN) if (FP + TN) > 0 else 0.0
        print(f"  {name:<20} → TPR = {TPR:.2f} | FPR = {FPR:.2f}")

    # 8. ROC One-vs-All
    if len(np.unique(y_test_int)) > 2 and y_score.shape[1] == len(class_names):
        y_bin = label_binarize(y_test_int, classes=np.arange(len(class_names)))

        plt.figure(figsize=(8, 6))
        for i, name in enumerate(class_names):
            if np.sum(y_bin[:, i]) == 0:
                print(f"Classe '{name}' absente de y_true → ROC ignoré.")
                continue
            fpr, tpr, _ = roc_curve(y_bin[:, i], y_score[:, i])
            auc = roc_auc_score(y_bin[:, i], y_score[:, i])
            plt.plot(fpr, tpr, label=f"{name} (AUC = {auc:.2f})")

        plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
        plt.title(f"Courbes ROC One-vs-All - {model_name}")
        plt.xlabel("Faux positifs (FPR)")
        plt.ylabel("Vrais positifs (TPR)")
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.show()
    else:
        print("ROC non générée (binaire ou score incompatible).")
#evaluate_model(model=mlp_clf, X_test=X_test_comb, y_test=y_test, le=le, model_name="MLPClassifier")
#evaluate_model(model=cnn_model, X_test=X_test_cnn, y_test=y_test_enc, le=le, model_name="CNN")

"""**Modelisation**

## **1. LLM4IDS Transformer**
"""

import tensorflow as tf
from tensorflow.keras import layers, models

def create_dense_transformer_multiclass(input_dim, num_classes):
    inputs = tf.keras.Input(shape=(1, input_dim))

    # Dense projection
    x = layers.Dense(256)(inputs)
    x = layers.LayerNormalization()(x)

    # Attention multi-têtes
    attn = layers.MultiHeadAttention(num_heads=4, key_dim=64)
    x_attn = attn(x, x)
    x = layers.Add()([x, x_attn])
    x = layers.LayerNormalization()(x)

    # Feedforward
    x = layers.Dense(128, activation='relu')(x)
    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dense(64, activation='relu')(x)
    outputs = layers.Dense(num_classes, activation='softmax')(x)

    return models.Model(inputs, outputs)

# Préparation des données
X_train_tf = np.expand_dims(X_train_comb, axis=1)
X_val_tf = np.expand_dims(X_val_comb, axis=1)
X_test_tf = np.expand_dims(X_test_comb, axis=1)

# Modèle
model = create_dense_transformer_multiclass(X_train_comb.shape[1], num_classes=5)
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003), loss='categorical_crossentropy', metrics=['accuracy',
                tf.keras.metrics.Precision(name='precision'),
                tf.keras.metrics.Recall(name='recall'),
                tf.keras.metrics.AUC(name='auc')])

# Callbacks
callbacks = [
    tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),
    tf.keras.callbacks.ModelCheckpoint("transformer_multiclass_nslkdd.h5", save_best_only=True)
]

# Entraînement
hist_llm4ids = model.fit(
    X_train_tf, y_train_enc,
    validation_data=(X_val_tf, y_val_enc),
    epochs=20,
    batch_size=32,
    callbacks=callbacks
)
# --------- Courbes d'apprentissage ---------
plt.figure(figsize=(12, 5))

# Courbe de la perte (Loss)
plt.subplot(1, 2, 1)
plt.plot(hist_llm4ids.history.get('loss', []), label='Train Loss')
plt.plot(hist_llm4ids.history.get('val_loss', []), label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Courbe de Perte - Transformer (LLM4IDS)')
plt.legend()
plt.grid(True)

# Courbe de la précision (Accuracy)
plt.subplot(1, 2, 2)
plt.plot(hist_llm4ids.history.get('accuracy', []), label='Train Accuracy')
plt.plot(hist_llm4ids.history.get('val_accuracy', []), label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title("Courbe de Précision - Transformer (LLM4IDS)")
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

#model.save_weights("LLM4-IDS/nsl-kdd/models/llm4ids_multiclass_nslkdd_weights.h5")
model.save("/content/drive/MyDrive/LLM4IDS/LLM4-IDS/nsl-kdd/Mod_Multi/llm4ids_multiclass_nslkdd2.h5")
model.save("/content/drive/MyDrive/LLM4IDS/LLM4-IDS/nsl-kdd/Mod_Multi/llm4ids_multiclass_nslkdd2.keras")

# Évaluation sur le test set
# Capture all metrics returned by model.evaluate()
test_results = model.evaluate(X_test_tf, y_test_enc)

# The metrics are returned in the order they were compiled.
# Assuming the order is ['loss', 'accuracy', 'precision', 'recall', 'auc']
# Access the accuracy by index (usually the second metric) or by name if available
test_loss = test_results[0]
test_acc = test_results[1] # Assuming accuracy is the second metric

print(f"Transformer Test Accuracy: {test_acc:.4f}")

# Prédictions
y_pred = model.predict(X_test_tf)
y_pred_labels = le.inverse_transform(np.argmax(y_pred, axis=1))

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred_labels))

from tensorflow.keras.models import load_model

#llm4ids_model = load_model("LLM4-IDS/nsl-kdd/models/llm4ids_multiclass_nslkdd.keras")
llm4ids_model = load_model("/content/drive/MyDrive/LLM4IDS/LLM4-IDS/nsl-kdd/Mod_Multi/llm4ids_multiclass_nslkdd2.h5")

eval_model_unv(model=llm4ids_model, X_test=X_test_tf, y_test=y_test_enc, le=le, model_name="LLM4IDS")

"""## **2. MLP Classifer**"""

from sklearn.neural_network import MLPClassifier

# 1. Entraînement
mlp_clf = MLPClassifier(
    hidden_layer_sizes=(256, 128),
    max_iter=300,
    activation='relu',
    solver='adam',
    random_state=42
)
mlp_clf.fit(X_train_comb, y_train)

# 2. Évaluation
print("Validation set (MLP)")
print(classification_report(y_val, mlp_clf.predict(X_val_comb)))

print("Test set (KDDTest+)")
print(classification_report(y_test, mlp_clf.predict(X_test_comb)))

mlp_clf = joblib.load("/content/drive/MyDrive/LLM4IDS/LLM4-IDS/nsl-kdd/models/mlp_sklearn_llm4ids.pkl")
eval_model_unv(model=mlp_clf, X_test=X_test_comb, y_test=y_test, le=le, model_name="MLPClassifier")

"""## **3. CNN**"""

# Entrée : vecteurs combinés reshaped
input_dim = X_train_comb.shape[1]
X_train_cnn = X_train_comb.reshape(-1, input_dim, 1)
X_val_cnn   = X_val_comb.reshape(-1, input_dim, 1)
X_test_cnn  = X_test_comb.reshape(-1, input_dim, 1)

from tensorflow.keras import models, layers, regularizers
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# Architecture CNN 1D améliorée
cnn_model = models.Sequential([
    layers.Input(shape=(input_dim, 1)),

    layers.Conv1D(64, 3, padding="same", activation="relu"),
    layers.BatchNormalization(),
    layers.Dropout(0.4),

    layers.Conv1D(128, 3, padding="same", activation="relu"),
    layers.BatchNormalization(),
    layers.Dropout(0.3),

    layers.Conv1D(64, 3, padding="same", activation="relu"),
    layers.BatchNormalization(),
    layers.GlobalAveragePooling1D(),

    layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),
    layers.Dropout(0.4),

    layers.Dense(y_train_enc.shape[1], activation='softmax')
])

cnn_model.compile(
    loss='categorical_crossentropy',
    optimizer=Adam(learning_rate=1e-3),
    metrics=['accuracy']
)

# Callbacks
callbacks = [
    EarlyStopping(patience=10, monitor="val_loss", restore_best_weights=True),
    ReduceLROnPlateau(patience=5, factor=0.5, min_lr=1e-6)
]

# Entraînement
cnn_h = cnn_model.fit(
    X_train_cnn, y_train_enc,
    validation_data=(X_val_cnn, y_val_enc),
    epochs=100,
    batch_size=128,
    callbacks=callbacks,
    verbose=1
)
# --------- Courbes d'apprentissage ---------
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 5))

# 1. Courbe de perte
plt.subplot(1, 2, 1)
plt.plot(cnn_h.history.get('loss', []), label='Train Loss')
plt.plot(cnn_h.history.get('val_loss', []), label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Courbe de Perte - CNN')
plt.legend()
plt.grid(True)

# 2. Courbe de précision
plt.subplot(1, 2, 2)
plt.plot(cnn_h.history.get('accuracy', []), label='Train Accuracy')
plt.plot(cnn_h.history.get('val_accuracy', []), label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title("Courbe de Précision - CNN")
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

#Save model
cnn_model.save("/content/drive/MyDrive/LLM4IDS/LLM4-IDS/nsl-kdd/Mod_Multi/cnn_multiclass.keras")

#Loard keras
from tensorflow.keras.models import load_model
cnn_model = load_model("/content/drive/MyDrive/LLM4IDS/LLM4-IDS/nsl-kdd/Mod_Multi/cnn_multiclass.keras")
eval_model_unv(model=cnn_model, X_test=X_test_cnn, y_test=y_test_enc, le=le, model_name="CNN")

from tensorflow.keras import models, layers, regularizers
from tensorflow.keras.optimizers import Adam
import tensorflow as tf # Add this line

# Dimensions d'entrée
input_dim = X_train_comb.shape[1]

# Architecture CNN 1D pour données tabulaires
cnn_model = models.Sequential([
    layers.Input(shape=(input_dim, 1)),  # Reshape préalable nécessaire
    layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),
    layers.BatchNormalization(),
    layers.Dropout(0.3),

    layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),
    layers.BatchNormalization(),
    layers.GlobalAveragePooling1D(),

    layers.Dense(64, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(y_train_enc.shape[1], activation='softmax')
])

# Compilation
cnn_model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003),
        loss='categorical_crossentropy',
        metrics=['accuracy',
                tf.keras.metrics.Precision(name='precision'),
                tf.keras.metrics.Recall(name='recall'),
                tf.keras.metrics.AUC(name='auc')]
    )
cnn_model.summary()

X_train_cnn = np.expand_dims(X_train_comb, axis=2)
X_val_cnn   = np.expand_dims(X_val_comb, axis=2)
X_test_cnn  = np.expand_dims(X_test_comb, axis=2)

# Exemple : entraînement
from tensorflow.keras.callbacks import ReduceLROnPlateau

lr_callback = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=3,
    verbose=1,
    min_lr=1e-6
)
cnn_h = cnn_model.fit(
    X_train_cnn, y_train_enc,
    validation_data=(X_val_cnn, y_val_enc),
    epochs=50,
    batch_size=128,
    verbose=1
)
# --------- Courbes d'apprentissage ---------
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 5))

# 1. Courbe de perte
plt.subplot(1, 2, 1)
plt.plot(cnn_h.history.get('loss', []), label='Train Loss')
plt.plot(cnn_h.history.get('val_loss', []), label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Courbe de Perte - CNN')
plt.legend()
plt.grid(True)

# 2. Courbe de précision
plt.subplot(1, 2, 2)
plt.plot(cnn_h.history.get('accuracy', []), label='Train Accuracy')
plt.plot(cnn_h.history.get('val_accuracy', []), label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title("Courbe de Précision - CNN")
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

#Save keras
cnn_model.save("/content/drive/MyDrive/LLM4IDS/LLM4-IDS/nsl-kdd/Mod_Multi/cnn_multiclass.keras")

#Loard keras
from tensorflow.keras.models import load_model
#cnn_model = load_model("LLM4-IDS/nsl-kdd/models/cnn_multiclass.keras")
cnn_model = load_model("/content/drive/MyDrive/LLM4IDS/LLM4-IDS/nsl-kdd/Mod_Multi/cnn_multiclass.keras")

eval_model_unv(model=cnn_model, X_test=X_test_cnn, y_test=y_test_enc, le=le, model_name="CNN")

"""## GRU"""

# Reshape for GRU
X_train = np.expand_dims(X_train_comb, axis=1)
X_val = np.expand_dims(X_val_comb, axis=1)
X_test_r  = np.expand_dims(X_test_comb, axis=1)

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, models, regularizers, callbacks
from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_selection import SelectKBest, mutual_info_classif
from sklearn.metrics import classification_report, confusion_matrix, roc_curve
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns

  # Reshape for GRU
X_train = np.expand_dims(X_train_comb, axis=1)
X_val = np.expand_dims(X_val_comb, axis=1)
X_test  = np.expand_dims(X_test_comb, axis=1)

# ===== 2. ENHANCED MODEL ARCHITECTURE =====
def build_enhanced_model(input_shape, num_classes):
    gru = models.Sequential([
        layers.Input(shape=input_shape),
        layers.Bidirectional(layers.GRU(160, return_sequences=True,
                                      kernel_regularizer=regularizers.l2(0.001))),
        layers.BatchNormalization(),
        layers.Dropout(0.3),
        layers.Bidirectional(layers.GRU(80,
                                      kernel_regularizer=regularizers.l2(0.001))),
        layers.BatchNormalization(),
        layers.Dropout(0.4),
        layers.Dense(80, activation='relu',
                   kernel_regularizer=regularizers.l2(0.001)),
        layers.Dropout(0.4),
        layers.Dense(num_classes, activation='softmax') # Use num_classes here
    ])

    gru.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003),
        loss='categorical_crossentropy',
        metrics=['accuracy',
                tf.keras.metrics.Precision(name='precision'),
                tf.keras.metrics.Recall(name='recall'),
                tf.keras.metrics.AUC(name='auc')]
    )
    return gru

# Use the number of classes from y_train_enc
num_classes = y_train_enc.shape[1]
gru = build_enhanced_model((1, X_train.shape[2]), num_classes)

#model = build_enhanced_model((1, X_train.shape[2]))
# Enhanced callbacks
callbacks = [
    callbacks.EarlyStopping(patience=8, monitor='val_auc',
                          mode='max', restore_best_weights=True),
    callbacks.ReduceLROnPlateau(factor=0.5, patience=4, min_lr=1e-6),
    callbacks.ModelCheckpoint('best_model.keras', monitor='val_auc',
                            save_best_only=True, mode='max')
]

history = gru.fit(
    X_train, y_train_enc, # Use y_train_enc here
    validation_data=(X_val, y_val_enc), # Use y_val_enc here
    epochs=100,  # Increased epochs
    batch_size=512,  # Larger batch size
    callbacks=callbacks,
    verbose=1
)

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history.get('loss', []), label="Train Loss")
plt.plot(history.history.get('val_loss', []), label="Val Loss")
plt.legend(), plt.title("Loss")

plt.subplot(1, 2, 2)
plt.plot(history.history.get('accuracy', []), label="Train Acc")
plt.plot(history.history.get('val_accuracy', []), label="Val Acc")
plt.legend(), plt.title("Accuracy")
plt.tight_layout()
plt.show()

#Save model keras
gru.save("/content/drive/MyDrive/LLM4IDS/LLM4-IDS/nsl-kdd/Mod_Multi/gru_multiclass.keras")

#Loard keras
from tensorflow.keras.models import load_model
gru = load_model("/content/drive/MyDrive/LLM4IDS/LLM4-IDS/nsl-kdd/Mod_Multi/gru_multiclass.keras")

eval_model_unv(model=gru, X_test=X_test, y_test=y_test_enc, le=le, model_name="GRU")

"""## **3. Random Forest**"""

# Import necessary libraries
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import ADASYN
from imblearn.pipeline import Pipeline # Import Pipeline from imblearn
import time  # Importer le module time
# Ensure imblearn is installed
try:
    from imblearn.over_sampling import ADASYN
except ImportError:
    print("imblearn not found. Installing...")
    !pip install imbalanced-learn
    from imblearn.over_sampling import ADASYN

# Define the pipeline using imblearn's Pipeline
# This allows the inclusion of sampling steps like ADASYN
pipeline = Pipeline([
    ('adasyn', ADASYN(random_state=42)),
    ('classifier', RandomForestClassifier(random_state=42))
])

param_dist = {
    'classifier__n_estimators': [100, 200, 300],
    'classifier__max_depth': [None, 10, 20, 30],
    'classifier__min_samples_split': [2, 5, 10],
    'classifier__min_samples_leaf': [1, 2, 4],
    'classifier__bootstrap': [True, False],
    'adasyn__n_neighbors': [3, 5, 7], # adasyn params are correctly specified here
}

rf_model = RandomizedSearchCV(
    estimator=pipeline,
    param_distributions=param_dist,
    n_iter=10, # Number of parameter settings that are sampled. Lower for faster run.
    cv=3,     # Number of folds in cross-validation
    verbose=2,
    n_jobs=-1, # Use all available CPU cores
    # Consider using a metric more appropriate for imbalanced data
    # like 'f1', 'recall', or 'roc_auc' instead of 'accuracy'
    scoring='roc_auc',
    random_state=42
)
start = time.time()
# Fit the model using the correct imblearn pipeline
rf_model.fit(X_train_comb, y_train)
finish = time.time()

t = finish - start

print("Best params:", rf_model.best_params_)
print("Best CV Score:", rf_model.best_score_)
print("Best Test Score:", rf_model.score(X_val_comb, y_val_enc))

#Save model
import joblib
import os
save_dir = "/content/drive/MyDrive/LLM4IDS/LLM4-IDS/nsl-kdd/models"
os.makedirs(save_dir, exist_ok=True)
rf_model_filename = "random_forest_llm4ids.pkl"
rf_model_path = os.path.join(save_dir, rf_model_filename)
joblib.dump
(rf_model.best_estimator_, rf_model_path)
print(f"Modèle Random Forest sauvegardé dans : {rf_model_path}")

#Loard rf
from sklearn.ensemble import RandomForestClassifier
import joblib
rf_loaded = joblib.load("/content/drive/MyDrive/LLM4IDS/LLM4-IDS/nsl-kdd/models/random_forest_llm4ids.pkl")
eval_model_unv(model=rf_loaded, X_test=X_test_comb, y_test=y_test, le=le, model_name="Random Forest")

from sklearn.ensemble import RandomForestClassifier

# 9. Entraînement Random Forest
rf_clf = RandomForestClassifier(n_estimators=200, class_weight="balanced", random_state=42)
rf_clf.fit(X_train_comb, y_train)


# 11. Évaluation
print(" Validation set (KDDTrain+)")
print(classification_report(y_val.astype(str), rf_clf.predict(X_val_comb).astype(str)))

#Save model rf_clf LLM4-IDS/nsl-kdd/models/
import joblib
import os
save_dir = "/content/drive/MyDrive/LLM4IDS/LLM4-IDS/nsl-kdd/models"
os.makedirs(save_dir, exist_ok=True)
rf_clf_filename = "random_forest_llm4ids.pkl"
rf_clf_path = os.path.join(save_dir, rf_clf_filename)
joblib.dump(rf_clf, rf_clf_path)
print(f"Modèle Random Forest sauvegardé dans : {rf_clf_path}")

#loard model
from sklearn.ensemble import RandomForestClassifier
import joblib
rf_loaded = joblib.load("/content/drive/MyDrive/LLM4IDS/LLM4-IDS/nsl-kdd/models/random_forest_llm4ids.pkl")

eval_model_unv(model=rf_loaded, X_test=X_test_comb, y_test=y_test, le=le, model_name="Random Forest")

"""## **4. XGBoost**"""

from xgboost import XGBClassifier
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sentence_transformers import SentenceTransformer
import pandas as pd
import numpy as np
import joblib
import matplotlib.pyplot as plt


# 12. Entraînement XGBoost
xgb_clf = XGBClassifier(
    objective="multi:softmax",
    num_class=len(le.classes_), # Use the number of unique classes from the LabelEncoder
    eval_metric="mlogloss",
    use_label_encoder=False,
    n_estimators=200,
    max_depth=6,
    learning_rate=0.1,
    random_state=42
)

# Fit with integer labels
xgb_clf.fit(X_train_comb, le.transform(y_train))

# 13. Évaluation
print("\n Validation set (KDDTrain+)")
y_val_pred = xgb_clf.predict(X_val_comb)
print(classification_report(y_val, le.inverse_transform(y_val_pred), target_names=le.classes_))

# 12. Sau
# 3. Sauvegarde
import joblib
import os

# === Dossier de sauvegarde
save_dir = "/content/drive/MyDrive/LLM4IDS/LLM4-IDS/nsl-kdd/models"
os.makedirs(save_dir, exist_ok=True)

# === Nom du fichier du modèle
xgb_filename = "xgb_llm4ids_model.pkl"
xgb_path = os.path.join(save_dir, xgb_filename)

# === Sauvegarde
joblib.dump(xgb_clf, xgb_path)
print(f"Modèle XGBoost sauvegardé dans : {xgb_path}")
joblib.dump(le, os.path.join(save_dir, "label_encoder.pkl"))

from xgboost import XGBClassifier
import joblib

# === Chargement
xgb_model = joblib.load("/content/drive/MyDrive/LLM4IDS/LLM4-IDS/nsl-kdd/models/xgb_llm4ids_model.pkl")
le = joblib.load(os.path.join(save_dir, "label_encoder.pkl"))

# === Prédiction
eval_model_unv(model=xgb_model, X_test=X_test_comb, y_test=y_test, le=le, model_name="XGBoost")

"""## 5. Logistic Regression"""

from sklearn.linear_model import LogisticRegression
# 10. Entraînement LogisticRegression (multiclasse)
lr_clf = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs')
lr_clf.fit(X_train_comb, y_train)

# 11. Évaluation
print(" Validation set (KDDTrain+)")
print(classification_report(y_val.astype(str), lr_clf.predict(X_val_comb).astype(str)))

eval_model_unv(model=lr_clf, X_test=X_test_comb, y_test=y_test, le=le, model_name="Logistic Regression")

import os
import joblib

# === Dossier de sauvegarde
save_dir = "/content/drive/MyDrive/LLM4IDS/LLM4-IDS/nsl-kdd/models"
os.makedirs(save_dir, exist_ok=True)

# === Nom du fichier
lr_model_filename = "logistic_regression_llm4ids.pkl"
lr_model_path = os.path.join(save_dir, lr_model_filename)

# === Sauvegarde du modèle
joblib.dump(lr_clf, lr_model_path)
print(f" Modèle Logistic Regression sauvegardé dans : {lr_model_path}")
joblib.dump(le, os.path.join(save_dir, "label_encoder.pkl"))

from sklearn.linear_model import LogisticRegression
import joblib

# === Chargement
lr_clf = joblib.load("/content/drive/MyDrive/LLM4IDS/LLM4-IDS/nsl-kdd/models/logistic_regression_llm4ids.pkl")
le = joblib.load(os.path.join(save_dir, "label_encoder.pkl"))

# === Utilisation
y_pred = lr_clf.predict(X_test_comb)

eval_model_unv(model=lr_clf, X_test=X_test_comb, y_test=y_test, le=le, model_name="Logistic Regression")



"""## **Comparaison de model**"""

eval_metrics = {
    "Modèle": [],
    "Seuil": [],
    "Accuracy": [],
    "Precision": [],
    "Recall": [],
    "F1-score": []
}

class_names = le.classes_

import tensorflow as tf
from tensorflow.keras import layers
import numpy as np
from tensorflow.keras.models import load_model
import joblib
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix
from sklearn.preprocessing import label_binarize
import re
import io
import sys

# Ensure models are loaded and le is available (from previous cells)
# llm4ids_model = load_model("/content/drive/MyDrive/LLM4IDS/LLM4-IDS/nsl-kdd/models/llm4ids_multiclass_nslkdd.keras")
llm4ids_model = load_model("/content/drive/MyDrive/LLM4IDS/LLM4-IDS/nsl-kdd/Mod_Multi/llm4ids_multiclass_nslkdd2.h5")
gru = load_model("/content/drive/MyDrive/LLM4IDS/LLM4-IDS/nsl-kdd/models/gru_multiclass.keras")
mlp_model = joblib.load("/content/drive/MyDrive/LLM4IDS/LLM4-IDS/nsl-kdd/models/mlp_sklearn_llm4ids.pkl")
#cnn_model = load_model("/content/drive/MyDrive/LLM4IDS/LLM4-IDS/nsl-kdd/models/cnn_multiclass.keras")
cnn_model = load_model("/content/drive/MyDrive/LLM4IDS/LLM4-IDS/nsl-kdd/Mod_Multi/cnn_multiclass.keras")
#rf_loaded = joblib.load("/content/drive/MyDrive/LLM4IDS/LLM4-IDS/nsl-kdd/models/random_forest_llm4ids.pkl")
rf_loaded = joblib.load("/content/drive/MyDrive/LLM4IDS/LLM4-IDS/nsl-kdd/models/random_forest_llm4ids.pkl")
xgb_model = joblib.load("/content/drive/MyDrive/LLM4IDS/LLM4-IDS/nsl-kdd/models/xgb_llm4ids_model.pkl")
lr_clf = joblib.load("/content/drive/MyDrive/LLM4IDS/LLM4-IDS/nsl-kdd/models/logistic_regression_llm4ids.pkl")
# le = joblib.load(os.path.join("/content/drive/MyDrive/LLM4IDS/LLM4-IDS/nsl-kdd/models", "label_encoder.pkl"))
#Loard rf

eval_metrics = {
    "Modèle": [],
    "Accuracy": [],
    "Precision": [],
    "Recall": [],
    "F1-score": []
}

# List of models to evaluate
models_to_evaluate = [
    {"name": "LLM4IDS", "model": llm4ids_model, "X_test": X_test_tf, "y_test": y_test_enc, "keras": True},
    {"name": "GRU", "model": gru, "X_test": X_test_r, "y_test": y_test_enc, "keras": True},
    {"name": "MLPClassifier", "model": mlp_model, "X_test": X_test_comb, "y_test": y_test, "keras": False},
    {"name": "CNN", "model": cnn_model, "X_test": X_test_cnn, "y_test": y_test_enc, "keras": True},
    {"name": "Random Forest", "model": rf_loaded, "X_test": X_test_comb, "y_test": y_test, "keras": False},
    {"name": "XGBoost", "model": xgb_model, "X_test": X_test_comb, "y_test": y_test, "keras": False},
    {"name": "Logistic Regression", "model": lr_clf, "X_test": X_test_comb, "y_test": y_test, "keras": False}
]

# Use eval_model_unv to get metrics and then collect them
for model_info in models_to_evaluate:
    name = model_info["name"]
    model = model_info["model"]
    X_test = model_info["X_test"]
    y_test_true = model_info["y_test"]
    is_keras = model_info["keras"]

    # Capture the classification report output
    old_stdout = sys.stdout
    sys.stdout = captured_output = io.StringIO()

    # Call the evaluation function (it prints the report)
    eval_model_unv(model=model, X_test=X_test, y_test=y_test_true, le=le, model_name=name)

    sys.stdout = old_stdout
    report = captured_output.getvalue()

    # Extract accuracy, precision, recall, and f1-score from the weighted avg line of the report
    weighted_avg_line = [line for line in report.split('\n') if 'weighted avg' in line]

    if weighted_avg_line:
        metrics = weighted_avg_line[0].split()
        # Assuming the format is 'weighted avg  precision  recall  f1-score  support'
        # We need to find the correct indices for precision, recall, and f1-score
        # The accuracy is on a separate line.

        # Extract accuracy from the accuracy line
        accuracy_line = [line for line in report.split('\n') if 'accuracy' in line]
        accuracy = float(accuracy_line[0].split()[-2]) if accuracy_line else None


        # Extract precision, recall, f1 from the weighted avg line
        # Need to find the index of 'weighted avg' and then the metrics that follow
        try:
            weighted_avg_index = metrics.index('avg') # Look for 'avg' which is part of 'weighted avg'
            prec = float(metrics[weighted_avg_index + 1]) if weighted_avg_index + 1 < len(metrics) else None
            rec = float(metrics[weighted_avg_index + 2]) if weighted_avg_index + 2 < len(metrics) else None
            f1 = float(metrics[weighted_avg_index + 3]) if weighted_avg_index + 3 < len(metrics) else None
        except (ValueError, IndexError):
            prec, rec, f1 = None, None, None


        # Store the collected metrics
        eval_metrics["Modèle"].append(name)
        eval_metrics["Accuracy"].append(accuracy)
        eval_metrics["Precision"].append(prec)
        eval_metrics["Recall"].append(rec)
        eval_metrics["F1-score"].append(f1)
    else:
         # Handle cases where the report format might be unexpected
        print(f"Could not extract weighted average metrics for {name}")
        eval_metrics["Modèle"].append(name)
        eval_metrics["Accuracy"].append(None)
        eval_metrics["Precision"].append(None)
        eval_metrics["Recall"].append(None)
        eval_metrics["F1-score"].append(None)


# Display the comparison table
metrics_df = pd.DataFrame(eval_metrics)
display(metrics_df)

# Plot the comparison
metrics_df.set_index("Modèle")[["Accuracy", "Precision", "Recall", "F1-score"]].plot(kind="bar", figsize=(12, 6))
plt.title(" Comparaison des performances des modèles IDS (Multiclass)")
plt.ylabel("Score")
plt.ylim(0, 1.1) # Adjusted y-limit for clarity
plt.xticks(rotation=45, ha="right")
plt.grid(axis='y')
plt.tight_layout()
plt.show()

from sklearn.metrics import classification_report
import numpy as np
import io
import sys
import pandas as pd
import re

eval_metrics = {
    "Model": [],
    "Accuracy": [],
    "Precision": [],
    "Recall": [],
    "F1-score": []
}

for model_info in models_to_evaluate:
    name = model_info["name"]
    model = model_info["model"]
    X_test = model_info["X_test"]
    y_test_true = model_info["y_test"]
    is_keras = model_info["keras"]

    # Prédictions
    if is_keras:
        y_pred = model.predict(X_test)
        y_pred_labels = np.argmax(y_pred, axis=1)
    else:
        y_pred_labels = model.predict(X_test)

    # Harmonisation des formats
    y_test_true = np.array(y_test_true)
    y_pred_labels = np.array(y_pred_labels)

    # Si y_test est one-hot
    if len(y_test_true.shape) > 1 and y_test_true.shape[1] > 1:
        y_test_true = np.argmax(y_test_true, axis=1)

    # Si y_test contient des chaînes
    if isinstance(y_test_true[0], str):
        y_test_true = le.transform(y_test_true)

    if isinstance(y_pred_labels[0], str):
        y_pred_labels = le.transform(y_pred_labels)

    # Capture du rapport
    old_stdout = sys.stdout
    sys.stdout = captured_output = io.StringIO()
    print(classification_report(y_test_true, y_pred_labels))
    sys.stdout = old_stdout
    report = captured_output.getvalue()

    # Extraction des lignes
    weighted_avg_line = [line for line in report.split('\n') if 'weighted avg' in line]
    accuracy_line = [line for line in report.split('\n') if 'accuracy' in line]

    try:
        accuracy = float(accuracy_line[0].split()[-2]) if accuracy_line else None
        if weighted_avg_line:
            metrics = re.split(r'\s+', weighted_avg_line[0].strip())
            precision = float(metrics[2])
            recall = float(metrics[3])
            f1 = float(metrics[4])
        else:
            precision = recall = f1 = None

        eval_metrics["Model"].append(name)
        eval_metrics["Accuracy"].append(accuracy)
        eval_metrics["Precision"].append(precision)
        eval_metrics["Recall"].append(recall)
        eval_metrics["F1-score"].append(f1)

    except Exception as e:
        print(f"Erreur pour {name} : {e}")
        eval_metrics["Model"].append(name)
        eval_metrics["Accuracy"].append(None)
        eval_metrics["Precision"].append(None)
        eval_metrics["Recall"].append(None)
        eval_metrics["F1-score"].append(None)

# Résultats
metrics_df = pd.DataFrame(eval_metrics)
display(metrics_df)

# Graphe
metrics_df.set_index("Model")[["Accuracy", "Precision", "Recall", "F1-score"]].plot(kind="bar", figsize=(12, 6))
plt.title("LLM4IDS Models comparison of performances (Multiclass)")
plt.ylabel("Score")
plt.ylim(0, 1.1)
plt.xticks(rotation=45, ha="right")
plt.grid(axis='y')
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

labels = ["Accuracy", "Precision", "Recall", "F1-score"]
num_vars = len(labels)

angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()
angles += angles[:1]  # boucle fermée

plt.figure(figsize=(8, 8))
for i, row in metrics_df.iterrows():
    values = row[labels].tolist()
    values += values[:1]
    plt.polar(angles, values, label=row["Modèle"], linewidth=2)

plt.xticks(angles[:-1], labels)
plt.yticks([0.2, 0.4, 0.6, 0.8, 1.0], color="gray")
plt.title("Radar chart – IDS model comparison")
plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))
plt.tight_layout()
plt.show()

# Exemple avec données multiples
long_df = pd.melt(metrics_df, id_vars="Modèle", var_name="Metric", value_name="Score")

plt.figure(figsize=(12, 6))
sns.boxplot(x="Metric", y="Score", hue="Modèle", data=long_df)
plt.title("Distribution des performances par métrique")
plt.grid(axis='y')
plt.tight_layout()
plt.show()

from sklearn.metrics import confusion_matrix
import seaborn as sns

# Exemple : confusion matrix du modèle CNN
y_true = np.argmax(y_test_enc, axis=1)
y_pred = np.argmax(cnn_model.predict(X_test_cnn), axis=1)

cm = confusion_matrix(y_true, y_pred)
labels = le.classes_ if hasattr(le, 'classes_') else ['Normal', 'DoS', 'Probe', 'R2L', 'U2R']

plt.figure(figsize=(7, 5))
sns.heatmap(cm, annot=True, fmt='d', xticklabels=labels, yticklabels=labels, cmap="YlGnBu")
plt.xlabel("Prédit")
plt.ylabel("Réel")
plt.title("Matrice de confusion – CNN")
plt.tight_layout()
plt.show()

from sklearn.metrics import confusion_matrix
import seaborn as sns

# Exemple : confusion matrix du modèle CNN
y_true = np.argmax(y_test_enc, axis=1)
y_pred = np.argmax(llm4ids_model.predict(X_test_tf), axis=1)

cm = confusion_matrix(y_true, y_pred)
labels = le.classes_ if hasattr(le, 'classes_') else ['Normal', 'DoS', 'Probe', 'R2L', 'U2R']

plt.figure(figsize=(7, 5))
sns.heatmap(cm, annot=True, fmt='d', xticklabels=labels, yticklabels=labels, cmap="YlGnBu")
plt.xlabel("Prédit")
plt.ylabel("Réel")
plt.title("Matrice de confusion – LLM4IDS")
plt.tight_layout()
plt.show()

"""### Evaluation2"""

def collect_scores(model, X_test, y_test, le, model_name, keras=False, model_path=None):
    # === 1. Prédiction
    if keras:
        y_score = model.predict(X_test, verbose=0)
        y_pred = np.argmax(y_score, axis=1)
        y_true = np.argmax(y_test, axis=1) if len(y_test.shape) > 1 else y_test
    else:
        y_pred = model.predict(X_test)
        y_true = y_test

    # === 2. Harmonisation des types
    if isinstance(y_true[0], str) and not isinstance(y_pred[0], str):
        y_pred = le.inverse_transform(y_pred)
    elif not isinstance(y_true[0], str) and isinstance(y_pred[0], str):
        y_true = le.inverse_transform(y_true)

    # === 3. Scores
    acc = accuracy_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred, average='weighted')

    # === 4. Inference time
    inf_time = measure_inference_time(model, X_test, keras=keras)

    # === 5. Taille du modèle
    size = get_model_size(model_path) if model_path else None

    return {
        "Modèle": model_name,
        "Accuracy": round(acc * 100, 2),
        "F1-score": round(f1 * 100, 2),
        "Temps inf. (ms/ex)": inf_time,
        "Taille (MB)": size
    }

import time
import numpy as np

def measure_inference_time(model, X, keras=False, n_repeats=3):
    times = []
    for _ in range(n_repeats):
        start = time.time()
        if keras:
            _ = model.predict(X, verbose=0)
        else:
            _ = model.predict(X)
        end = time.time()
        times.append(end - start)
    avg_total_time = np.mean(times)
    return round(avg_total_time / X.shape[0] * 1000, 3)  # en millisecondes / échantillon

import os

def get_model_size(filepath):
    if filepath and os.path.exists(filepath):
        return round(os.path.getsize(filepath) / (1024 * 1024), 2)  # MB
    else:
        return None

from sklearn.metrics import classification_report
import io
import sys

scores = []
# Exemple : X_test est de forme (n_samples, n_features)

# List of models to evaluate
models_to_evaluate = [
    {"name": "LLM4IDS", "model": llm4ids_model, "X_test": X_test_tf, "y_test": y_test_enc, "keras": True, "model_path": "LLM4-IDS/nsl-kdd/models/llm4ids_multiclass_nslkdd.keras"},
    {"name": "GRU", "model": gru, "X_test": X_test, "y_test": y_test_enc, "keras": True, "model_path": "LLM4-IDS/nsl-kdd/models/gru_multiclass.keras"},
    {"name": "MLPClassifier", "model": mlp_model, "X_test": X_test_comb, "y_test": y_test, "keras": False, "model_path": "LLM4-IDS/nsl-kdd/models/mlp_sklearn_llm4ids.pkl"},
    {"name": "Random Forest", "model": rf_loaded, "X_test": X_test_comb, "y_test": y_test, "keras": False, "model_path": "LLM4-IDS/nsl-kdd/models/random_forest_llm4ids.pkl"},
    {"name": "XGBoost", "model": xgb_model, "X_test": X_test_comb, "y_test": y_test, "keras": False, "model_path": "LLM4-IDS/nsl-kdd/models/xgb_llm4ids_model.pkl"},
    {"name": "Logistic Regression", "model": lr_clf, "X_test": X_test_comb, "y_test": y_test, "keras": False, "model_path": "LLM4-IDS/nsl-kdd/models/logistic_regression_llm4ids.pkl"}
]

# Use eval_model_unv to get metrics and then collect them
for model_info in models_to_evaluate:
    name = model_info["name"]
    model = model_info["model"]
    X_test = model_info["X_test"]
    y_test_true = model_info["y_test"]
    is_keras = model_info["keras"]
    model_path = model_info["model_path"]

    # Capture the classification report output
    old_stdout = sys.stdout
    sys.stdout = captured_output = io.StringIO()

    eval_model_unv(model=model, X_test=X_test, y_test=y_test_true, le=le, model_name=name)

    sys.stdout = old_stdout
    report = captured_output.getvalue()

    # Extract accuracy and weighted avg f1-score from the report
    accuracy_line = [line for line in report.split('\n') if 'accuracy' in line]
    f1_weighted_line = [line for line in report.split('\n') if 'weighted avg' in line]

    accuracy = float(accuracy_line[0].split()[-2]) if accuracy_line else None
    f1_weighted = float(f1_weighted_line[0].split()[-2]) if f1_weighted_line else None

    # Get inference time and model size
    inf_time = measure_inference_time(model, X_test, keras=is_keras)
    size = get_model_size(model_path)

    scores.append({
        "Modèle": name,
        "Accuracy": round(accuracy * 100, 2) if accuracy is not None else None,
        "F1-score": round(f1_weighted * 100, 2) if f1_weighted is not None else None,
        "Temps inf. (ms/ex)": inf_time,
        "Taille (MB)": size
    })

import matplotlib.pyplot as plt
import seaborn as sns

df_scores = pd.DataFrame(scores)

# Accuracy plot
plt.figure(figsize=(10, 5))
sns.barplot(x="Modèle", y="Accuracy", data=df_scores, palette="viridis")
plt.title("Comparaison des modèles - Accuracy (%)")
plt.ylim(0, 105)
plt.xticks(rotation=45)
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# F1-score plot
plt.figure(figsize=(10, 5))
sns.barplot(x="Modèle", y="F1-score", data=df_scores, palette="magma")
plt.title("Comparaison des modèles - F1-score (%)")
plt.ylim(0, 105)
plt.xticks(rotation=45)
plt.grid(axis='y')
plt.tight_layout()
plt.show()

from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import numpy as np

def plot_best_model_per_class_roc(models_info, X_test_dict, y_test, le):
    """
    Affiche une courbe ROC par classe, avec le meilleur modèle pour chaque.

    Parameters:
    - models_info : list of dicts with keys:
        - "name": nom du modèle
        - "model": l'objet modèle
        - "keras": True/False
        - "X_test": données d'entrée
    - y_test : ground truth (one-hot or int)
    - le : LabelEncoder utilisé
    """

    # 1. Préparation
    if len(y_test.shape) > 1:
        y_true = np.argmax(y_test, axis=1)
    else:
        y_true = y_test

    y_bin = label_binarize(y_true, classes=np.arange(len(le.classes_)))
    best_aucs = [-1] * len(le.classes_)
    best_curves = [None] * len(le.classes_)
    best_models = [""] * len(le.classes_)

    # 2. Pour chaque modèle, calculer les courbes ROC par classe
    for info in models_info:
        name = info["name"]
        model = info["model"]
        keras = info["keras"]
        X = info["X_test"]

        if keras:
            y_score = model.predict(X, verbose=0)
        elif hasattr(model, "predict_proba"):
            y_score = model.predict_proba(X)
        else:
            continue

        for i, class_name in enumerate(le.classes_):
            if np.sum(y_bin[:, i]) == 0:
                continue
            fpr, tpr, _ = roc_curve(y_bin[:, i], y_score[:, i])
            auc = roc_auc_score(y_bin[:, i], y_score[:, i])

            if auc > best_aucs[i]:
                best_aucs[i] = auc
                best_curves[i] = (fpr, tpr)
                best_models[i] = name

    # 3. Tracer les meilleures courbes par classe
    plt.figure(figsize=(10, 7))
    for i, class_name in enumerate(le.classes_):
        if best_curves[i] is not None:
            fpr, tpr = best_curves[i]
            auc = best_aucs[i]
            plt.plot(fpr, tpr, label=f"{class_name} (AUC={auc:.2f}, {best_models[i]})")

    plt.plot([0, 1], [0, 1], linestyle="--", color="gray")
    plt.title("Courbe ROC - Meilleur modèle par classe")
    plt.xlabel("Faux positifs (FPR)")
    plt.ylabel("Vrais positifs (TPR)")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

def plot_model_comparison(df_scores):
    sns.set(style="whitegrid")
    fig, axes = plt.subplots(2, 2, figsize=(16, 10))
    fig.suptitle(" Comparaison des Modèles LLM4IDS (NSL-KDD)", fontsize=16, fontweight='bold')

    # 1. Accuracy
    sns.barplot(x="Accuracy", y="Modèle", data=df_scores, ax=axes[0, 0], palette="viridis")
    axes[0, 0].set_title(" Accuracy (%)")
    axes[0, 0].set_xlim(0, 105)

    # 2. F1-score
    sns.barplot(x="F1-score", y="Modèle", data=df_scores, ax=axes[0, 1], palette="magma")
    axes[0, 1].set_title(" F1-score (%)")
    axes[0, 1].set_xlim(0, 105)

    # 3. Temps d’inférence
    sns.barplot(x="Temps inf. (ms/ex)", y="Modèle", data=df_scores, ax=axes[1, 0], palette="crest")
    axes[1, 0].set_title("⏱ Temps d'inférence (ms/exemple)")

    # 4. Taille du modèle
    sns.barplot(x="Taille (MB)", y="Modèle", data=df_scores, ax=axes[1, 1], palette="flare")
    axes[1, 1].set_title(" Taille du modèle (MB)")

    # Mise en forme
    for ax in axes.flat:
        ax.bar_label(ax.containers[0], fmt="%.2f", label_type='edge')
        ax.set_ylabel("")
        ax.grid(axis='x', linestyle='--', alpha=0.5)

    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.show()

plot_model_comparison(df_scores)

import joblib
import tensorflow as tf
import numpy as np

# Dictionnaire de résultats
eval_metrics = {
    "Modèle": [],
    "Accuracy": [],
    "Precision": [],
    "Recall": [],
    "F1-score": []
}

import pandas as pd

df_results = pd.DataFrame(eval_metrics)
df_results.sort_values("F1-score", ascending=False, inplace=True)
display(df_results)

metrics_df = pd.DataFrame(eval_metrics)
display(metrics_df)

metrics_df.set_index("Modèle")[["Accuracy", "Precision", "Recall", "F1-score"]].plot(kind="bar", figsize=(12, 6))
plt.title("Comparaison des performances des modèles IDS")
plt.ylabel("Score")
plt.ylim(0, 1)
plt.xticks(rotation=45, ha="right")
plt.grid(axis='y')
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# 1. Accuracy
plt.figure(figsize=(10, 5))
sns.barplot(x="Modèle", y="Accuracy", data=df_scores, palette="viridis")
plt.title("Accuracy (%) par modèle")
plt.ylim(0, 105)
plt.xticks(rotation=45)
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# 2. F1-score
plt.figure(figsize=(10, 5))
sns.barplot(x="Modèle", y="F1-score", data=df_scores, palette="magma")
plt.title("F1-score (%) par modèle")
plt.ylim(0, 105)
plt.xticks(rotation=45)
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# 3. Temps d'inférence
plt.figure(figsize=(10, 5))
sns.barplot(x="Modèle", y="Temps inf. (ms/ex)", data=df_scores, palette="crest")
plt.title("Temps d'inférence moyen (ms/échantillon)")
plt.xticks(rotation=45)
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# 4. Taille du modèle
plt.figure(figsize=(10, 5))
sns.barplot(x="Modèle", y="Taille (MB)", data=df_scores, palette="flare")
plt.title("Taille du modèle sur disque (MB)")
plt.xticks(rotation=45)
plt.grid(axis='y')
plt.tight_layout()
plt.show()

from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import numpy as np

def plot_best_model_per_class_roc(models_info, X_test_dict, y_test, le):
    """
    Affiche une courbe ROC par classe, avec le meilleur modèle pour chaque.

    Parameters:
    - models_info : list of dicts with keys:
        - "name": nom du modèle
        - "model": l'objet modèle
        - "keras": True/False
        - "X_test": données d'entrée (correctly shaped for the model)
    - y_test : ground truth (one-hot or int)
    - le : LabelEncoder utilisé
    """

    # 1. Préparation
    if len(y_test.shape) > 1:
        y_true = np.argmax(y_test, axis=1)
    else:
        y_true = y_test

    y_bin = label_binarize(y_true, classes=np.arange(len(le.classes_)))
    best_aucs = [-1] * len(le.classes_)
    best_curves = [None] * len(le.classes_)
    best_models = [""] * len(le.classes_)

    # 2. Pour chaque modèle, calculer les courbes ROC par classe
    for info in models_info:
        name = info["name"]
        model = info["model"]
        keras = info["keras"]
        X = info["X_test"] # Use the pre-shaped X_test from the dictionary

        # Ensure correct shape for Keras models before prediction
        if keras:
            # Check the expected input shape of the Keras model
            input_shape = model.input_shape
            if len(input_shape) == 3 and len(X.shape) == 2:
                 # Add the missing dimension if needed (e.g., for GRU/Transformer)
                 X_shaped = np.expand_dims(X, axis=1)
            elif len(input_shape) == 3 and len(X.shape) == 3 and input_shape[2] == 1 and X.shape[2] != 1:
                 # Reshape if the feature dimension is missing but time step/channel is present (e.g., for CNN)
                 X_shaped = np.expand_dims(X, axis=2)
            else:
                X_shaped = X # Use as is if shapes are compatible

            y_score = model.predict(X_shaped, verbose=0)
        elif hasattr(model, "predict_proba"):
            y_score = model.predict_proba(X)
        else:
            continue

        for i, class_name in enumerate(le.classes_):
            if np.sum(y_bin[:, i]) == 0:
                continue
            fpr, tpr, _ = roc_curve(y_bin[:, i], y_score[:, i])
            auc = roc_auc_score(y_bin[:, i], y_score[:, i])

            if auc > best_aucs[i]:
                best_aucs[i] = auc
                best_curves[i] = (fpr, tpr)
                best_models[i] = name

    # 3. Tracer les meilleures courbes par classe
    plt.figure(figsize=(10, 7))
    for i, class_name in enumerate(le.classes_):
        if best_curves[i] is not None:
            fpr, tpr = best_curves[i]
            auc = best_aucs[i]
            plt.plot(fpr, tpr, label=f"{class_name} (AUC={auc:.2f}, {best_models[i]})")

    plt.plot([0, 1], [0, 1], linestyle="--", color="gray")
    plt.title("Courbe ROC - Meilleur modèle par classe")
    plt.xlabel("Faux positifs (FPR)")
    plt.ylabel("Vrais positifs (TPR)")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

from tensorflow.keras.models import load_model
import joblib

models_info = [
    {
        "name": "LLM4IDS",
        "model": load_model("/content/drive/MyDrive/LLM4IDS/LLM4-IDS/nsl-kdd/Mod_Multi/llm4ids_multiclass_nslkdd2.keras"),
        "keras": True,
        "X_test": X_test_tf
    },
    {
        "name": "CNN",
        "model": load_model("/content/drive/MyDrive/LLM4IDS/LLM4-IDS/nsl-kdd/models/cnn_multiclass.keras"),
        "keras": True,
        "X_test": X_test_cnn
    },
    {
        "name": "MLP",
        "model": joblib.load("/content/drive/MyDrive/LLM4IDS/LLM4-IDS/nsl-kdd/models/mlp_sklearn_llm4ids.pkl"),
        "keras": False,
        "X_test": X_test_comb
    },
    {
        "name": "Random Forest",
        "model": joblib.load("/content/drive/MyDrive/LLM4IDS/LLM4-IDS/nsl-kdd/models/random_forest_llm4ids.pkl"),
        "keras": False,
        "X_test": X_test_comb
    },
    {
        "name": "XGBoost",
        "model": joblib.load("/content/drive/MyDrive/LLM4IDS/LLM4-IDS/nsl-kdd/models/xgb_llm4ids_model.pkl"),
        "keras": False,
        "X_test": X_test_comb
    },
    {
        "name": "GRU",
        "model": load_model("/content/drive/MyDrive/LLM4IDS/LLM4-IDS/nsl-kdd/models/gru_multiclass.keras"),
        "keras": True,
        "X_test": X_test
    }
]

# y_test_enc = one-hot ou y_test = int selon ton code
plot_best_model_per_class_roc(models_info, X_test_dict=None, y_test=y_test_enc, le=le)

"""### Interpretability"""

!pip install ipywidgets
!jupyter nbextension enable --py widgetsnbextension

import ipywidgets as widgets
from IPython.display import display
import matplotlib.pyplot as plt
import seaborn as sns

# Dropdown pour choisir un modèle
model_selector = widgets.Dropdown(
    options=metrics_df["Modèle"].tolist(),
    description='Modèle:',
    style={'description_width': 'initial'}
)

# Fonction pour afficher les scores du modèle sélectionné
def plot_model_metrics(model_name):
    row = metrics_df[metrics_df["Modèle"] == model_name].iloc[0]
    scores = row[["Accuracy", "Precision", "Recall", "F1-score"]]

    plt.figure(figsize=(6, 4))
    sns.barplot(x=scores.index, y=scores.values, palette="viridis")
    plt.title(f"Performance du modèle : {model_name}")
    plt.ylim(0, 1.1)
    plt.grid(axis='y')
    plt.tight_layout()
    plt.show()

# Lier l’interactivité
interactive_plot = widgets.interactive_output(
    plot_model_metrics,
    {'model_name': model_selector}
)

# Affichage du tableau de bord
display(widgets.VBox([model_selector, interactive_plot]))

pip install shap

pip install lime

from lime import lime_tabular
import numpy as np

X_train_tf = np.expand_dims(X_train_comb, axis=1)
X_val_tf = np.expand_dims(X_val_comb, axis=1)
X_test_tf = np.expand_dims(X_test_comb, axis=1)
# Important : on retire les dimensions inutiles (LLM4IDS = (n, 1, features))
X_lime = X_test_tf[:, 0, :]  # (n_samples, n_features)

# 1. Créer l’explainer
explainer = lime_tabular.LimeTabularExplainer(
    training_data=X_lime,
    mode='classification',
    feature_names=[f"f{i}" for i in range(X_lime.shape[1])],
    class_names=le.classes_,  # par ex. ['Normal', 'DoS', 'Probe', 'R2L', 'U2R']
    discretize_continuous=True
)

# 2. Choisir une instance à expliquer
i = 42  # indice à expliquer
instance = X_lime[i]

# 3. Fonction prédictive adaptée pour LIME
def predict_fn_lime(data):
    data_reshaped = data.reshape((data.shape[0], 1, data.shape[1]))  # (batch, 1, features)
    return llm4ids_model.predict(data_reshaped)

# 4. Explication
exp = explainer.explain_instance(instance, predict_fn_lime, num_features=10)

# 5. Affichage
exp.show_in_notebook(show_table=True, show_all=False)

!pip install lime